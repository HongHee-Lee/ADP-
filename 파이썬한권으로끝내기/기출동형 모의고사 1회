# -*- coding: utf-8 -*-
"""
Created on Sun Sep  1 20:27:22 2024

@author: USER
"""

import pandas as pd
import numpy as np
df=pd.read_csv("https://raw.githubusercontent.com/ADPclass/ADP_book_ver01/main/data/student_data.csv")
df
df.info()
display(df)
"""
예전에 빅데이터 분석기사 공부할때 저장해둔 코드들로 빠르게 기억 복원하기!
https://github.com/HongHee-Lee/Bigdata_Analysis_Engineer_practical-test/blob/main/220617.%ED%98%BC%EA%B3%B5.%EC%9E%91%EC%97%85%ED%98%952%EC%9C%A0%ED%98%95.%EC%B5%9C%EC%A2%85.%EC%8B%A4%EA%B8%B03%ED%9A%8C
"""
pd.set_option('display.max_columns', None)    #전체 컬럼을 다보여주는 코드
df

import matplotlib.pyplot as plt

plt.bar(df['grade'].value_counts().index, df['grade'].value_counts().values)
#,반드시 있어야 한다. 안그러면 에러 발생한다
# SyntaxError: invalid syntax. Perhaps you forgot a comma?

import scipy.stats as stats
stats.shapiro(df["grade"])
"""샤피로 윌크 검정 결과 pvalue인 유의확률은 0.0014 정도로 나온다
통계량이 임계값보다 작으면 귀무 가설을 기각하고, 데이터가 정규 분포를 따르지 않는 것으로 판단됩니다.
 https://blog.naver.com/microsample/223250843551
그니까 샤피로 윌크 검정은 정규성을 따르는지를 알아보는 검정방법인데, 
대립가설이 틀릴확률이 0.0014라는 것이다. 
즉, 대립가설이 틀릴확률이 너무 낮고 이말은 귀무가설이 참일 확률이 너무 낮다는 것이다.
따라서 귀무가설을 기각하고 대립가설을 채택한다. 여기서 귀무가설은 데이터가 정규분포를 따른다는 것이다.
결론적으로 우리는 귀무가설을 기각하고 데이터가 정규분포를 따르지 않는다고 결론을 내릴수 있다.
"""

"""
import seaborn as sns
df_cor=df.corr(method='pearson')
#위코드 실행하면 ValueError: could not convert string to float: 'GP' 뜬다  
#데이터에 영어 등 숫자 외의 문자 값이 포함되어있어서 그렇다.
sns.heatmap(df_cor,
            xticklabels=df_cor.columns,
            yticklabels=df_cor.columns,
            cmap='RdBu_r',
            annot=True,
            linewidth=3)
"""

#범주형 변수의 시각화
fig, axs=plt.subplots(2,2)
axs[0][0].bar(df['school'].value_counts().index,
              df['school'].value_counts().values)
#여기 까지만 하면 사분면중에 2사분면(직관적으로 첫칸)에만 표가 그려진다
axs[0][1].bar(df['sex'].value_counts().index,
              df['sex'].value_counts().values)
axs[1][0].bar(df['paid'].value_counts().index,
              df['paid'].value_counts().values)
axs[1][1].bar(df['activities'].value_counts().index,
              df['activities'].value_counts().values)



"""
<결측값 개수 확인 2가지 방법>
1.결측값 개수 확인 코드는 df.isnull().sum() 임  df.isnull().count()가 아님

2.
df1 = df['goout'].value_counts(dropna=False)
df1

goout
3.0    128
2.0    103
4.0     86
5.0     53
1.0     22
NaN      3
Name: count, dtype: int64

nan 항목을 통해 개별 칼럼의 결측값 개수도 알수 있음
"""
df.isnull().sum()

#2.결측치를 식별, 예측하는 두 가지 방법을 쓰고, 이를 선택한 이유를 설명하시오.
"""수치형 변수만 결측치가 있으므로 KNN기법을 사용하여 결측치를 대체한다.
KNN 기법을 통해 모든 결측치를 대체해본다."""
df[df.isna().any(axis=1)]

from sklearn.impute import KNNImputer

#결측치가 있는 수치형 데이터만을 추출
KNN_data=df.drop(columns=['school','sex', 'paid','activities'])

#모델링
imputer=KNNImputer()
df_filled=imputer.fit_transform(KNN_data)
df_filled=pd.DataFrame(df_filled, columns=KNN_data.columns)
df[KNN_data.columns]=df_filled

#결측치 확인
df.isna().sum()


#범주형 변수 인코딩이 필요한 경우
#school, sex, paid, activities -> 이산형 / 나머지는 수치나 순위형이므로 원핫인코딩x
df=pd.get_dummies(data=df, columns=['school','sex','paid','activities'],drop_first=True)
df.info()

#데이터 분할 방법
#랜덤분할(train, test로 내가 지정한 비율로 분할), 층화추출 기법
from sklearn.model_selection import train_test_split
X=df.drop('grade', axis=1)
y=df['grade']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2022)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

#svm, xgboost, randomforest 의 공통점과 성적 예측 분석에 적합한 알고리즘인가 설명?
"""
종속변수의 값이 연속형이므로 회귀분석이 적합 
세개모두 회귀분석, 분류분석 가능. 회귀분석시 svm은 커널트릭을통해,
xgboost와 randomforest는 트리모델을 통해 다중공선성을 해결할 수 있다.
성적이라는 연속형 변수를 예츨하기에 3가지 모델은 적합하다
"""

#svm, xgboost,randomforest세가지모델 모델링해보기
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC

scaler=StandardScaler()
X_train_scaled=pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)
X_test_scaled=pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)

#SVM
"""svm의 경우는 매개변수를 디테일하게 찾아 모델 적용 . 정확도 높아짐.
이에 반해 Randomforest와 XGBoost는 비교적 간단하게 그리드서치를 통해 매개변수를 적용시킴."""
from sklearn.model_selection import GridSearchCV
param_grid=[{'C':[0.1,1,10,100], 'gamma':[0.001,0.01,0.1,1,10]}]
grid_svm=GridSearchCV(SVR(), param_grid=param_grid, cv=5)
grid_svm.fit(X_train_scaled, y_train)
result=pd.DataFrame(grid_svm.cv_results_['params'])
result['mean_test_score']=grid_svm.cv_results_['mean_test_score']
result.sort_values(by='mean_test_score', ascending=False)

#SVM
svr=SVR(C=100,gamma=0.001)
svr.fit(X_train_scaled, y_train)
print("R2 : ", svr.score(X_test_scaled, y_test))
print("RMSE :", np.sqrt(mean_squared_error(y_test, svr.predict(X_test_scaled))))
#R2 :  0.9574163452579304
#RMSE : 0.7753004545895887

#randomforest
rf_grid=[{'max_depth': [2,4,6,8,10], 'min_samples_split': [2,4,6,8,10]}]
rf=GridSearchCV(RandomForestRegressor(n_estimators=100), param_grid=rf_grid, cv=5)
rf.fit(X_train, y_train)
print(rf.best_params_)
print("R2 : ", rf.score(X_test, y_test))
print("RMSE : ", np.sqrt(mean_squared_error(y_test, rf.predict(X_test))))
#{'max_depth': 8, 'min_samples_split': 2}
#R2 :  0.9528899966058348
#RMSE :  0.8154646519486668

#xgboost
xgb_grid=[{'max_depth':[2,4,6,8,10]}]
xgb=GridSearchCV(XGBRegressor(n_estimators=1000), param_grid=xgb_grid, cv=5)
xgb.fit(X_train, y_train)
xgb.score(X_test, y_test)
print("R : ", xgb.score(X_test, y_test))
print("RMSE : ",np.sqrt(mean_squared_error(y_test, xgb.predict(X_test))))
#R :  0.9569358092115691
#RMSE :  0.7796626290745525

"""
아래는 변수중요도를 보여주는 코드 3개인데
ValueError: tree must be Booster, XGBModel or dict instance
이런 메세지가 뜬다 
결국 아직 해결못함...

1.
from xgboost import plot_importance
plot_importance(xgb)

2.
import matplotlib.pyplot as plt
%matplotlib inline
fig, ax = plt.subplots(figsize=(10, 12))
plot_importance(xgb, ax=ax)

3.
from xgboost import plot_importance
from matplotlib import pyplot
plot_importance(xgb)
pyplot.show()"""
